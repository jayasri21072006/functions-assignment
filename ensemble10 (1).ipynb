{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H3UUDQKE0gUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical"
      ],
      "metadata": {
        "id": "D2oNugcR0tmO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f504c6b"
      },
      "source": [
        "\n",
        "\n",
        "**1: Can we use Bagging for regression problems?**\n",
        "Yes, Bagging can be used for regression problems. It is called Bagging Regressor.\n",
        "\n",
        "**2: What is the difference between multiple model training and single model training?**\n",
        "Single model training involves training one model on the entire dataset. Multiple model training, or ensemble learning, involves training multiple models (often of the same type) on different subsets of the data or with different parameters, and then combining their predictions.\n",
        "\n",
        "**3: Explain the concept of feature randomness in Random Forest.**\n",
        "Feature randomness (also called feature bagging or random subspaces) in Random Forest means that at each split in a decision tree, only a random subset of the features is considered. This adds diversity to the trees and helps to reduce correlation between them, further reducing overfitting.\n",
        "\n",
        "**4: What is OOB (Out-of-Bag) Score?**\n",
        "In Bagging, since each model is trained on a bootstrap sample, some data points are left out of the training set for each individual model. These left-out data points form the \"out-of-bag\" (OOB) sample. The OOB score is the average prediction error of the ensemble on the OOB samples, which can be used as an estimate of the model's performance without needing a separate validation set.\n",
        "\n",
        "**5: How can you measure the importance of features in a Random Forest model?**\n",
        "Feature importance in Random Forest can be measured by calculating the average reduction in impurity (like Gini impurity or entropy) or the average increase in error (like mean squared error for regression) across all trees when a feature is used for splitting. Features that contribute more to reducing impurity or increasing error are considered more important.\n",
        "\n",
        "**6: Explain the working principle of a Bagging Classifier.**\n",
        "A Bagging Classifier works by training multiple base classifiers (usually decision trees) on different bootstrap samples of the training data. For classification, the final prediction is made by taking a majority vote of the predictions from all the base classifiers.\n",
        "\n",
        "**7: How do you evaluate a Bagging Classifier’s performance?**\n",
        "A Bagging Classifier’s performance can be evaluated using standard classification metrics such as accuracy, precision, recall, F1-score, and AUC on a separate test set. The OOB score can also be used as an internal estimate of performance during training.\n",
        "\n",
        "**8: How does a Bagging Regressor work?**\n",
        "A Bagging Regressor works similarly to a Bagging Classifier, but for regression problems. It trains multiple base regressors (usually decision trees) on different bootstrap samples. The final prediction is typically the average of the predictions from all the base regressors.\n",
        "\n",
        "**9: What is the main advantage of ensemble techniques?**\n",
        "The main advantage of ensemble techniques is that they can often achieve better performance and are more robust than single models, especially in terms of reducing variance and preventing overfitting.\n",
        "\n",
        "**10: What is the main challenge of ensemble methods?**\n",
        "The main challenge of ensemble methods can be their increased computational cost due to training multiple models. They can also be more complex to interpret than single models.\n",
        "\n",
        "**11: Explain the key idea behind ensemble techniques.**\n",
        "The key idea behind ensemble techniques is to combine the predictions of multiple individual models to produce a more accurate and robust prediction than any single model could achieve on its own. By combining diverse models, the ensemble can reduce the impact of individual model weaknesses and biases.\n",
        "\n",
        "**12: What is a Random Forest Classifier?**\n",
        "A Random Forest Classifier is an ensemble learning method that builds multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It combines the concepts of Bagging and feature randomness.\n",
        "\n",
        "**13: What are the main types of ensemble techniques?**\n",
        "The main types of ensemble techniques are Bagging (Bootstrap Aggregating), Boosting, and Stacking.\n",
        "\n",
        "**14: What is ensemble learning in machine learning?**\n",
        "Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained and their predictions are combined to solve a particular problem.\n",
        "\n",
        "**15: When should we avoid using ensemble methods?**\n",
        "We might avoid using ensemble methods when computational resources are severely limited, when interpretability of the model is paramount, or when the dataset is very small and simple, as the overhead of training multiple models might not be worth the potential performance gain.\n",
        "\n",
        "**16: How does Bagging help in reducing overfitting?**\n",
        "Bagging helps in reducing overfitting by reducing the variance of the model. By training multiple models on different bootstrap samples, it averages out the errors and variations that might occur in individual models trained on slightly different data.\n",
        "\n",
        "**17: Why is Random Forest better than a single Decision Tree?**\n",
        "Random Forest is generally better than a single Decision Tree because it reduces overfitting and improves generalization. Single decision trees can easily overfit to the training data, while Random Forest, by combining multiple trees trained on different data subsets and features, provides a more robust and accurate model.\n",
        "\n",
        "**18: What is the role of bootstrap sampling in Bagging?**\n",
        "Bootstrap sampling is crucial in Bagging as it creates multiple diverse training datasets from the original dataset by sampling with replacement. Each base model in the ensemble is trained on one of these bootstrap samples. This diversity in the training data for each model helps to reduce the correlation between the models and improve the overall ensemble performance.\n",
        "\n",
        "**19: What are some real-world applications of ensemble techniques?**\n",
        "Ensemble techniques are widely used in various real-world applications, including:\n",
        "*   Image recognition and computer vision\n",
        "*   Natural Language Processing (NLP) tasks like sentiment analysis and text classification\n",
        "*   Fraud detection\n",
        "*   Medical diagnosis\n",
        "*   Recommendation systems\n",
        "*   Predictive maintenance\n",
        "\n",
        "**20: What is the difference between Bagging and Boosting?**\n",
        "The main difference between Bagging and Boosting lies in how they train and combine the base models.\n",
        "*   **Bagging** trains base models independently and in parallel on different bootstrap samples of the data. The final prediction is typically an average or majority vote of the base models. Bagging primarily reduces variance.\n",
        "*   **Boosting** trains base models sequentially, where each subsequent model focuses on correcting the errors made by the previous models. It typically assigns higher weights to misclassified or poorly predicted data points. Boosting primarily reduces bias and can also reduce variance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eM99WR4v12Mj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94a347be"
      },
      "source": [
        "**21: Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d764e5b",
        "outputId": "9290ed52-af0d-4746-a7b2-ddc5422b4f9b"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2a76f49"
      },
      "source": [
        "**22: Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31be39bc",
        "outputId": "f0c86f87-e12e-4e7b-84b8-9574921bad0d"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 9462.992186060874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d11bee"
      },
      "source": [
        "**23: Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00467a1b",
        "outputId": "d383accc-6ce7-4453-c626-f6534a2f737d"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "importance_series = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Sort feature importances and print\n",
        "sorted_importance = importance_series.sort_values(ascending=False)\n",
        "print(\"Feature Importances:\")\n",
        "print(sorted_importance)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "worst area                 0.153892\n",
            "worst concave points       0.144663\n",
            "mean concave points        0.106210\n",
            "worst radius               0.077987\n",
            "mean concavity             0.068001\n",
            "worst perimeter            0.067115\n",
            "mean perimeter             0.053270\n",
            "mean radius                0.048703\n",
            "mean area                  0.047555\n",
            "worst concavity            0.031802\n",
            "area error                 0.022407\n",
            "worst texture              0.021749\n",
            "worst compactness          0.020266\n",
            "radius error               0.020139\n",
            "mean compactness           0.013944\n",
            "mean texture               0.013591\n",
            "perimeter error            0.011303\n",
            "worst smoothness           0.010644\n",
            "worst symmetry             0.010120\n",
            "concavity error            0.009386\n",
            "mean smoothness            0.007285\n",
            "fractal dimension error    0.005321\n",
            "compactness error          0.005253\n",
            "worst fractal dimension    0.005210\n",
            "texture error              0.004724\n",
            "smoothness error           0.004271\n",
            "symmetry error             0.004018\n",
            "mean fractal dimension     0.003886\n",
            "mean symmetry              0.003770\n",
            "concave points error       0.003513\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb5c4f76"
      },
      "source": [
        "**24: Train a Random Forest Regressor and compare its performance with a single Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cf0bf18",
        "outputId": "2da46bfd-7e78-4de3-dd49-d8005571c714"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "print(f\"Single Decision Tree Regressor MSE: {dt_mse}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse}\")\n",
        "\n",
        "# Compare performance\n",
        "if rf_mse < dt_mse:\n",
        "    print(\"Random Forest Regressor performed better than a single Decision Tree Regressor.\")\n",
        "else:\n",
        "    print(\"Single Decision Tree Regressor performed better or equal to Random Forest Regressor.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Regressor MSE: 21352.575144570023\n",
            "Random Forest Regressor MSE: 8592.244973007591\n",
            "Random Forest Regressor performed better than a single Decision Tree Regressor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2e52ec"
      },
      "source": [
        "**25: Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "840feec1",
        "outputId": "1cbcc837-4bf1-4d56-fd3f-e449c4366e4a"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with oob_score=True\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf_oob.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB score\n",
        "oob_score = rf_clf_oob.oob_score_\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Random Forest Classifier OOB Score: {oob_score}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier OOB Score: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "113e5ac2"
      },
      "source": [
        "**26: Train a Bagging Classifier using SVM as a base estimator and print accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84abe5fb",
        "outputId": "647ada71-a59b-4486-f679-991be29168d6"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier with SVM Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fea22b"
      },
      "source": [
        "**27: Train a Random Forest Classifier with different numbers of trees and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64e9d88c",
        "outputId": "067ac810-b890-4bdb-8c8e-ca9adb4f1b87"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest with different numbers of trees\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest Classifier with {n_estimators} trees Accuracy: {accuracy}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier with 10 trees Accuracy: 0.855\n",
            "Random Forest Classifier with 50 trees Accuracy: 0.88\n",
            "Random Forest Classifier with 100 trees Accuracy: 0.9\n",
            "Random Forest Classifier with 200 trees Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0acaf60"
      },
      "source": [
        "**28: Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e95a1eab",
        "outputId": "e0db79f3-3490-4fb6-eefb-8e8b4d1223ab"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as base estimator\n",
        "# Note: LogisticRegression needs probability predictions for AUC\n",
        "bagging_lr = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier with Logistic Regression AUC Score: 0.9225203497135966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cfc155f"
      },
      "source": [
        "**29: Train a Random Forest Regressor and analyze feature importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46e53d06",
        "outputId": "aad3d771-ae9a-4f6d-aba8-8df52b665236"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_reg.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "importance_series = pd.Series(feature_importances, index=[f'feature_{i}' for i in range(X.shape[1])]) # Using generic names as make_regression doesn't provide feature names\n",
        "\n",
        "# Sort feature importances and print\n",
        "sorted_importance = importance_series.sort_values(ascending=False)\n",
        "print(\"Feature Importances:\")\n",
        "print(sorted_importance)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "feature_16    0.278247\n",
            "feature_4     0.200561\n",
            "feature_15    0.145842\n",
            "feature_5     0.125427\n",
            "feature_2     0.101418\n",
            "feature_18    0.022312\n",
            "feature_1     0.016148\n",
            "feature_10    0.009990\n",
            "feature_17    0.009499\n",
            "feature_12    0.009324\n",
            "feature_11    0.009227\n",
            "feature_14    0.009062\n",
            "feature_13    0.008921\n",
            "feature_9     0.008675\n",
            "feature_8     0.008085\n",
            "feature_6     0.008013\n",
            "feature_7     0.007857\n",
            "feature_19    0.007358\n",
            "feature_0     0.007162\n",
            "feature_3     0.006872\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6f881a2"
      },
      "source": [
        "**30: Train an ensemble model using both Bagging and Random Forest and compare accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "850da112",
        "outputId": "5ff8052c-06b1-4e21-ee12-1e6f31a7be67"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")\n",
        "\n",
        "# Compare performance\n",
        "if rf_accuracy > bagging_accuracy:\n",
        "    print(\"Random Forest Classifier performed better than Bagging Classifier.\")\n",
        "elif bagging_accuracy > rf_accuracy:\n",
        "    print(\"Bagging Classifier performed better than Random Forest Classifier.\")\n",
        "else:\n",
        "    print(\"Bagging Classifier and Random Forest Classifier performed equally.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.885\n",
            "Random Forest Classifier Accuracy: 0.9\n",
            "Random Forest Classifier performed better than Bagging Classifier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9e759d"
      },
      "source": [
        "**31: Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9891a435",
        "outputId": "17137f92-8acc-49e4-cbb4-19d1f29361b7"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters and best score\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best accuracy score: {grid_search.best_score_}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
            "Best accuracy score: 0.89375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b5b6d88"
      },
      "source": [
        "**32: Train a Bagging Regressor with different numbers of base estimators and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52cb9cba",
        "outputId": "035a4914-f21f-42a6-9246-8b4122cce417"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with different numbers of base estimators\n",
        "n_estimators_list = [10, 50, 100]\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {n_estimators} estimators MSE: {mse}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with 10 estimators MSE: 9462.992186060874\n",
            "Bagging Regressor with 50 estimators MSE: 8308.576076264717\n",
            "Bagging Regressor with 100 estimators MSE: 8517.426228499578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "572eaa86"
      },
      "source": [
        "**33: Train a Random Forest Classifier and analyze misclassified samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cb7aafa",
        "outputId": "22ef2cea-cd9e-4af4-ce13-58f5687ce891"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "print(\"Indices of misclassified samples:\")\n",
        "print(misclassified_indices)\n",
        "\n",
        "# You can further analyze these misclassified samples, e.g.,\n",
        "# print(\"Misclassified samples (features):\")\n",
        "# print(X_test[misclassified_indices])\n",
        "# print(\"True labels of misclassified samples:\")\n",
        "# print(y_test[misclassified_indices])\n",
        "# print(\"Predicted labels of misclassified samples:\")\n",
        "# print(y_pred[misclassified_indices])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified samples: 20\n",
            "Indices of misclassified samples:\n",
            "[  2   8  19  27  30  43  45  57  59  94  99 107 112 127 128 159 163 171\n",
            " 173 184]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47846c71"
      },
      "source": [
        "**34: Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4faa30",
        "outputId": "e8e0ae26-9448-41d2-b6b9-25a91ba2d48a"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "print(f\"Single Decision Tree Classifier Accuracy: {dt_accuracy}\")\n",
        "\n",
        "# Train a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Compare performance\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"Bagging Classifier performed better than a single Decision Tree Classifier.\")\n",
        "elif dt_accuracy > bagging_accuracy:\n",
        "    print(\"Single Decision Tree Classifier performed better than Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"Bagging Classifier and Single Decision Tree Classifier performed equally.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Classifier Accuracy: 0.875\n",
            "Bagging Classifier Accuracy: 0.885\n",
            "Bagging Classifier performed better than a single Decision Tree Classifier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ac8b89"
      },
      "source": [
        "**35: Train a Random Forest Classifier and visualize the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "105ccd96",
        "outputId": "4d1b4544-c0a3-458f-8059-f1a3634faaa9"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix for Random Forest Classifier\")\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "662e2232"
      },
      "source": [
        "**36: Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8e3b63c",
        "outputId": "b1ea974a-57b2-46b1-c07f-2d9d9706e8f7"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)), # SVM needs probability=True for stacking\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "# The final_estimator is the model that combines the predictions of the base estimators\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42), cv=5)\n",
        "\n",
        "# Train the model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8a1718"
      },
      "source": [
        "**37: Train a Random Forest Classifier and print the top 5 most important features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ca1012",
        "outputId": "29982e6b-1095-4ca1-b385-d6a3a344f95a"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_clf.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "importance_series = pd.Series(feature_importances, index=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "# Sort feature importances and print the top 5\n",
        "sorted_importance = importance_series.sort_values(ascending=False)\n",
        "print(\"Top 5 Feature Importances:\")\n",
        "print(sorted_importance.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Feature Importances:\n",
            "feature_5     0.363134\n",
            "feature_18    0.177252\n",
            "feature_1     0.101820\n",
            "feature_14    0.075575\n",
            "feature_11    0.025690\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a2b5fb"
      },
      "source": [
        "**38: Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e79a63",
        "outputId": "fc5aa23c-0a23-4579-f1ab-2a4a70a98485"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Bagging Classifier Precision: {precision}\")\n",
        "print(f\"Bagging Classifier Recall: {recall}\")\n",
        "print(f\"Bagging Classifier F1-score: {f1}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Precision: 0.9565217391304348\n",
            "Bagging Classifier Recall: 0.822429906542056\n",
            "Bagging Classifier F1-score: 0.8844221105527639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9118960a"
      },
      "source": [
        "**39: Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3603f722",
        "outputId": "e5e14664-cf28-4a83-a97f-5395e0518ef5"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest with different max_depth values\n",
        "max_depth_list = [None, 5, 10, 20]\n",
        "for max_depth in max_depth_list:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest Classifier with max_depth={max_depth} Accuracy: {accuracy}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier with max_depth=None Accuracy: 0.9\n",
            "Random Forest Classifier with max_depth=5 Accuracy: 0.88\n",
            "Random Forest Classifier with max_depth=10 Accuracy: 0.885\n",
            "Random Forest Classifier with max_depth=20 Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc733c41"
      },
      "source": [
        "**40: Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6266d6d",
        "outputId": "75cc3f5f-0e67-4e0d-93a7-036d2f8d23fb"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_dt_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_dt_reg.fit(X_train, y_train)\n",
        "bagging_dt_pred = bagging_dt_reg.predict(X_test)\n",
        "bagging_dt_mse = mean_squared_error(y_test, bagging_dt_pred)\n",
        "print(f\"Bagging Regressor with Decision Tree MSE: {bagging_dt_mse}\")\n",
        "\n",
        "# Train a Bagging Regressor with KNeighbors as base estimator\n",
        "bagging_knn_reg = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_knn_reg.fit(X_train, y_train)\n",
        "bagging_knn_pred = bagging_knn_reg.predict(X_test)\n",
        "bagging_knn_mse = mean_squared_error(y_test, bagging_knn_pred)\n",
        "print(f\"Bagging Regressor with KNeighbors Regressor MSE: {bagging_knn_mse}\")\n",
        "\n",
        "# Compare performance\n",
        "if bagging_dt_mse < bagging_knn_mse:\n",
        "    print(\"Bagging Regressor with Decision Tree performed better than with KNeighbors Regressor.\")\n",
        "elif bagging_knn_mse < bagging_dt_mse:\n",
        "    print(\"Bagging Regressor with KNeighbors Regressor performed better than with Decision Tree.\")\n",
        "else:\n",
        "    print(\"Bagging Regressor with Decision Tree and KNeighbors Regressor performed equally.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with Decision Tree MSE: 9462.992186060874\n",
            "Bagging Regressor with KNeighbors Regressor MSE: 14832.180009089792\n",
            "Bagging Regressor with Decision Tree performed better than with KNeighbors Regressor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fd47ef2"
      },
      "source": [
        "**41: Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9cb5780",
        "outputId": "606d5b3e-a135-49c0-f4a1-e586480be036"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print ROC-AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {auc_score}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier ROC-AUC Score: 0.9379459350819013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529a8733"
      },
      "source": [
        "**42: Train a Bagging Classifier and evaluate its performance using cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b406fa8f",
        "outputId": "ba200261-d187-43e5-f4e5-d65741bce090"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(f\"Bagging Classifier Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Bagging Classifier Mean Cross-Validation Accuracy: {cv_scores.mean()}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Cross-Validation Scores: [0.95  0.9   0.895 0.895 0.86 ]\n",
            "Bagging Classifier Mean Cross-Validation Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ead21788"
      },
      "source": [
        "**43: Train a Random Forest Classifier and plot the Precision-Recall curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "b7165a10",
        "outputId": "921cce72-2b7e-4315-9921-388be876ccc1"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve for Random Forest Classifier\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bca50415"
      },
      "source": [
        "**44: Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e41dc3f",
        "outputId": "0eb7e10e-07bb-455d-c420-2caeb9848df9"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=42), cv=5)\n",
        "\n",
        "# Train the model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70585c1e"
      },
      "source": [
        "**45: Train a Bagging Regressor with different levels of bootstrap samples and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dc82e64",
        "outputId": "2e4fa2a0-96e5-4874-ca66-255d20f2ac77"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different max_samples values for bootstrap\n",
        "max_samples_list = [0.5, 0.7, 1.0] # 0.5 means 50% of samples, 1.0 means 100%\n",
        "\n",
        "for max_samples in max_samples_list:\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, max_samples=max_samples, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with max_samples={max_samples} MSE: {mse}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with max_samples=0.5 MSE: 11858.297162932005\n",
            "Bagging Regressor with max_samples=0.7 MSE: 9761.713451886364\n",
            "Bagging Regressor with max_samples=1.0 MSE: 9462.992186060874\n"
          ]
        }
      ]
    }
  ]
}